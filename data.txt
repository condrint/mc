  The Foundations  of Artificial  Intelligence11Mathematics (c. 800-present)Philosophers  staked  out  most  of the  important  ideas  of  AI,  but  to  make  the  leap  to  a  formalscience required a level  of mathematical formalization in three  main  areas:  computation,  logic,ALGORITHM              and  probability.   The  notion  of expressing  a  computation  as  a  formal  algorithm  goes  back  toal-Khowarazmi,  an  Arab  mathematician  of the  ninth  century,  whose  writings  also  introducedEurope to Arabic numerals and algebra.Logic  goes back  at least to Aristotle, but it  was  a philosophical rather than  mathematicalsubject  until  George  Boole  (1815-1864)  introduced  his  formal  language  for  making  logicalinference  in  1847.  Boole's  approach  was  incomplete,  but  good enough that  others  filled in  thegaps.   In  1879,  Gottlob  Frege  (1848-1925)  produced  a  logic  that,  except  for  some  notationalchanges, forms the first-order logic that is used today as the most basic knowledge representationsystem.8   Alfred  Tarski  (1902-1983) introduced  a  theory  of reference  that  shows  how  to  relatethe  objects  in  a logic  to objects in the  real  world.  The  next  step  was  to determine  the limits  ofwhat could be done with logic and computation.David  Hilbert (1862-1943), a great mathematician  in his own right,  is most rememberedfor the problems  he  did not  solve.  In  1900,  he presented  a list  of 23  problems that he  correctlypredicted  would  occupy  mathematicians  for  the  bulk  of  the  century.    The  final  problem  asksif there  is  an  algorithm  for  deciding  the  truth  of any  logical  proposition involving  the  naturalnumbers—the  famous  Entscheidungsproblem,  or  decision  problem.    Essentially,  Hilbert  wasasking  if there  were fundamental limits to the power of effective proof procedures.  In  1930,  KurtGodel  (1906-1978) showed that there exists an effective procedure to prove any true statement inthe first-order logic of Frege and Russell;  but first-order logic could not capture the principle ofmathematical induction needed to characterize the natural numbers.  In  1931, he showed that realTNHCEora=METENESS       limits  do  exist.   His  incompleteness  theorem  showed that  in  any  language  expressive  enoughto describe the properties  of the natural  numbers,  there  are true statements  that  are undecidable:their truth cannot be established by any  algorithm.This fundamental  result  can  also be interpreted  as  showing that there  are  some functionson  the  integers  that  cannot  be  represented  by  an  algorithm—that is,  they  cannot  be  computed.This  motivated  Alan  Turing  (1912-1954)  to  try  to  characterize  exactly  which  functions  arecapable  of being  computed.   This  notion  is  actually  slightly  problematic,  because  the  notionof  a  computation  or  effective  procedure  really  cannot  be  given  a  formal  definition.   However,the  Church-Turing  thesis,  which  states  that  the  Turing  machine  (Turing,  1936)  is  capable  ofcomputing  any  computable  function,  is  generally  accepted  as  providing a  sufficient  definition.Turing  also  showed that  there  were  some  functions that  no  Turing  machine  can  compute.   Forexample,  no  machine  can  tell  in  general  whether  a  given  program  will  return  an  answer  on  agiven input,  or run forever.Although undecidability and noncomputability are important to an understanding of com-WTRACTABILITY          putation,  the  notion  of  intractability  has  had  a  much  greater  impact.     Roughly  speaking,a  class  of  problems  is  called  intractable  if  the  time  required  to  solve  instances  of  the  classgrows  at  least exponentially with  the  size  of the  instances.  The distinction between polynomialand  exponential  growth  in  complexity  was  first  emphasized  in  the  mid-1960s  (Cobham,  1964;Edmonds,  1965).  It is important because exponential growth means that even moderate-sized in-To understand why Frege's notation was not universally adopted, see the cover of this book.
    Introductionstances cannot be solved in any reasonable time.  Therefore, one should strive to divide the overallproblem of generating intelligent behavior into tractable subproblems rather than intractable ones.REDUCTION               The  second important concept  in the theory  of complexity  is  reduction,  which also emerged  inthe  1960s  (Dantzig,  1960;  Edmonds,  1962).   A  reduction  is  a  general  transformation from  oneclass  of problems to another,  such that solutions to the first class  can  be found by reducing themto problems  of the second class  and solving the latter problems.NP  COMPLETENESS                 How can one recognize an intractable problem?  The theory of NP-completeness, pioneeredby  Steven  Cook  (1971) and  Richard Karp  (1972), provides  a method.  Cook and  Karp  showedthe  existence  of  large  classes  of  canonical  combinatorial  search  and  reasoning  problems  thatare  NP-complete.   Any  problem  class  to which  an  NP-complete  problem  class  can  be  reducedis  likely  to  be  intractable.   (Although  it  has  not  yet  been  proved  that  NP-complete  problemsare  necessarily  intractable,  few  theoreticians  believe  otherwise.)   These  results  contrast  sharplywith the "Electronic Super-Brain" enthusiasm accompanying the advent of computers.  Despitethe  ever-increasing  speed  of computers,  subtlety  and  careful  use  of resources  will  characterizeintelligent systems.  Put crudely, the world is an extremely large problem instance!Besides  logic  and computation,  the third great contribution of mathematics  to  AI  is  the jtheory  of  probability.    The  Italian  Gerolamo  Cardano  (1501-1576)  first  framed  the  idea  of Iprobability, describing it in terms of the possible outcomes of gambling events.  Before his time, jthe outcomes of gambling games were seen as the will of the gods rather than the whim of chance, iProbability quickly  became  an  invaluable  part  of all  the  quantitative  sciences,  helping  to  dealwith uncertain measurements and incomplete theories. Pierre Fermat (1601-1665), Blaise Pascal I(1623-1662), James Bernoulli (1654-1705), Pierre Laplace  (1749-1827), and others advanced jthe  theory  and  introduced  new  statistical  methods.   Bernoulli  also  framed  an  alternative  view]of probability,  as  a  subjective  "degree  of  belief"  rather  than  an  objective  ratio  of  outcomes.!Subjective probabilities therefore can  be updated as  new evidence is obtained.  Thomas  Bayes j(1702-1761) proposed  a rule  for updating subjective probabilities in  the light  of new  evidence!(published posthumously in  1763).  Bayes'  rule,  and  the  subsequent  field  of Bayesian  analysis,!form the basis  of the modern  approach to uncertain reasoning in AI systems.  Debate  still rages jbetween  supporters  of the  objective  and  subjective views  of probability, but it is  not clear  if the!difference  has  great  significance  for  AI.  Both  versions  obey  the  same  set  of axioms.   Savage'sJ(1954) Foundations of Statistics gives a good introduction to the field.As  with  logic,  a  connection  must  be  made  between  probabilistic reasoning  and  action.!DECISION  THEORY       Decision  theory,  pioneered  by  John  Von  Neumann  and  Oskar  Morgenstern  (1944),  combines!probability  theory  with  utility  theory  (which  provides  a  formal  and  complete  framework  forlspecifying the preferences  of an  agent)  to give  the  first  general theory that  can distinguish good!actions  from  bad  ones.   Decision  theory  is  the  mathematical  successor  to  utilitarianism,  and]provides the theoretical basis for many of the agent designs in this book.Psychology (1879-present)Scientific psychology can be said to have begun with the work of the German physicist Hermann ivon  Helmholtz  (1821-1894) and  his  student Wilhelm Wundt  (1832-1920).  Helmholtz  appliedthe  scientific  method  to  the  study  of human  vision,  and  his  Handbook  of Physiological  Optics \
  The Foundations  of Artificial Intelligence13BEHAVIORISMCOGNITIVEPSYCHOLOGYis  even  now  described  as  "the  single  most  important  treatise  on  the physics  and  physiology  ofhuman  vision to this day" (Nalwa,  1993, p.15).  In  1879, the same year that Frege launched first-order  logic,  Wundt  opened  the  first  laboratory  of experimental  psychology  at  the  University  ofLeipzig.  Wundt insisted on carefully controlled experiments in which his workers would performa  perceptual  or  associative  task  while  introspecting  on  their  thought  processes.    The  carefulcontrols went a long way to make psychology a science, but as the methodology spread, a curiousphenomenon  arose:  each  laboratory would report introspective data that just happened to matchthe  theories  tint were  popular in that  laboratory.  The  behaviorism  movement  of John  Watson(1878-1958) aid Edward Lee Thorndike (1874-1949) rebelled against this subjectivism, rejectingany theory involving mental processes  on the grounds that introspection could not provide reliableevidence.  Behiviorists insisted on studying only objective measures  of the percepts  (or stimulus)given to  an  animal  and its resulting actions  (or response).  Mental constructs such as knowledge,beliefs, goals, md reasoning steps were dismissed as unscientific "folkpsychology."  Behaviorismdiscovered a let about rats and pigeons, but had less success understanding humans.  Nevertheless,it had a stronghold on psychology (especially in the United States)  from about  1920 to  1960.The  view  that the  brain possesses  and  processes  information,  which  is  the  principal  char-acteristic  of cognitive psychology,  can  be  traced  back  at  least  to the  works  of William James9(1842-1910).  Helmholtz also insisted that perception involved a form of unconscious logical in-ference.  The cognitive viewpoint was largely eclipsed by behaviorism until  1943, when  KennethCraik  published  The  Nature  of Explanation.   Craik  put  back  the  missing  mental  step  betweenstimulus and response.  He  claimed that beliefs,  goals,  and reasoning  steps could be useful  validcomponents  of a theory  of human behavior,  and  are just  as  scientific  as,  say,  using pressure  andtemperature  to talk  about gases,  despite  their being  made  of molecules  that have neither.  Craikspecified the tlree key steps of a knowledge-based agent:  (1) the stimulus must be translated intoan internal representation, (2) the representation is manipulated by cognitive processes to derivenew  internal  representations,  and  (3)  these  are in  turn retranslated back  into  action.  He  clearlyexplained  why this was  a good design for an agent:If the orgmism carries a "small-scale model" of external reality and of its own possible actionswithin  its head, it  is  able  to  try  out various alternatives,  conclude which  is  the best  of them,react to fiture situations before they arise, utilize the knowledge of past events in dealing withthe  present and future,  and in  every  way to react  in  a much fuller,  safer,  and more  competentmanner to the emergencies which face it.  (Craik,  1943)An  agent  designed  this  way  can,  for  example,  plan  a  long  trip  by  considering  various  possi-ble  routes,  comparing  them,  and  choosing  the  best  one,  all  before  starting the journey.   Sincethe  1960s,  the information-processing view  has  dominated psychology.  It  it now  almost takenfor  granted  among  many  psychologists  that  "a  cognitive  theory  should  be  like  a  computer  pro-gram" (Andersen,  1980).  By this it is meant that the theory should describe cognition as consistingof well-definej transformation processes  operating  at the  level  of the information carried by  theinput signals.For  most  of  the  early  history  of  AI  and  cognitive  science,  no  significant  distinction  wasdrawn  between the two fields,  and it was common to  see AI programs described as psychological9   William  James was the brother  of novelist Henry James.  It  is  said that Henry  wrote  fiction  as  if it  were psychologyand William  wrot; psychology as  if it were  fiction.
results without any claim as to the exact human behavior they were modelling.  In the last decadeor  so,  however,  the  methodological distinctions have  become  clearer,  and  most  work now fallsinto one  field  or the  other.Computer engineering (1940-present)For  artificial  intelligence  to  succeed,  we  need  two  things:   intelligence  and  an  artifact.    Thecomputer has been unanimously acclaimed  as the artifact with the best  chance  of demonstratingintelligence.   The  modern  digital  electronic  computer  was  invented  independently  and  almostsimultaneously  by  scientists  in  three  countries  embattled  in  World  War  II.  The  first  operationalmodern computer was the  Heath  Robinson,10 built in  1940 by Alan Turing's team for the singlepurpose of deciphering German  messages.  When the Germans switched to a more sophisticatedcode,  the  electromechanical  relays  in the  Robinson proved  to  be  too  slow,  and  a  new  machinecalled  the Colossus  was  built from  vacuum  tubes.  It  was  completed in  1943,  and by  the  end  ofthe war, ten Colossus machines were in everyday use.The  first  operational programmable  computer  was  the  Z-3,  the  invention  of  Konrad  Zusein Germany  in  1941.  Zuse  invented floating-point numbers for the Z-3,  and went on in  1945 todevelop Plankalkul,  the  first  high-level  programming  language.   Although  Zuse  received  somesupport from the Third Reich to apply his  machine  to  aircraft design,  the  military hierarchy  didnot attach  as  much importance to computing as did its counterpart in Britain.In  the  United  States,  the  first  electronic  computer,  the  ABC,  was  assembled  by  JohnAtanasoff and his graduate student Clifford Berry between  1940 and  1942 at Iowa State University.The project received little support and was abandoned after Atanasoff became involved in militaryresearch  in Washington.  Two  other computer projects  were  started  as  secret  military  research:the Mark I, If, and III computers were developed at Harvard by a team under Howard Aiken; andthe ENIAC was developed  at the University of Pennsylvania by a team including John Mauchlyand John  Eckert.  ENIAC  was  the  first general-purpose,  electronic,  digital  computer.  One  of itsfirst  applications  was  computing  artillery firing tables.  A  successor,  the  EDVAC,  followed JohnVon Neumann's suggestion to use a stored program,  so that technicians would not have to scurryabout changing patch cords to run a new program.But perhaps the  most critical  breakthrough was  the IBM 701,  built in  1952 by  NathanielRochester and his  group.  This  was  the first computer to yield a profit for its manufacturer.  IBMwent on to become  one of the world's largest corporations, and sales of computers have grown to j$150 billion/year.  In the United States,  the computer industry (including software and  services) jnow accounts for about  10%  of the gross national product.Each  generation of computer hardware has brought an increase in speed and capacity,  and Ia decrease in price.  Computer engineering has been remarkably  successful,  regularly doubling jperformance every two years,  with no immediate end in sight for this rate of increase.  Massively jparallel machines promise to add  several more zeros to the overall throughput achievable.Of course,  there  were  calculating  devices  before  the  electronic  computer.   The  abacus   \is  roughly  7000  years  old.   In  the  mid-17th century,  Blaise  Pascal  built  a  mechanical  adding 110  Heath  Robinson was  a cartoonist famous  for his  depictions  of whimsical  and  absurdly complicated contraptions foreveryday tasks such as buttering toast.
   The  Foundations  of Artificial  Intelligence                                                                                                  15and  subtracting  machine  called  the  Pascaline.    Leibniz  improved  on  this  in  1694.  building  amechanical  device that multiplied by doing repeated addition.  Progress  stalled for over  a centuryunti 1 Charles Babbage (1792-1871) dreamed that logarithm tables could be computed by machine.He  designed  a  machine  for this  task,  but  never  completed  the  project.   Instead,  he  turned  to  thedesign  of the  Analytical  Engine,  for  which  Babbage  invented  the  ideas  of addressable  memory.stored  programs,  and  conditional jumps.   Although  the  idea  of  programmable  machines  wasnot  new—in   1805.  Joseph  Marie  Jacquard  invented  a  loom  that  could  be  programmed  usingpunched cards—Babbage's machine  was the first artifact possessing the characteristics necessaryfor universal computation.  Babbage's colleague Ada Lovelace,  daughter of the poet Lord Byron,wrote  programs for the Analytical Engine and  even  speculated that the machine could play chessor  compose  music.   Lovelace  was  the  world's  first  programmer,  and  the  first  of  many  to  enduremassive cost overruns and to have  an ambitious project ultimately abandoned."  Babbage's basicdesign was  proven  viable by  Doron  Swade and his colleagues,  who built  a working model  usingonly  the  mechanical  techniques  available  at  Babbage's  time  (Swade.  1993).   Babbage  had  theright  idea,  but  lacked the organizational  skills to  get his  machine built.AI  also  owes  a  debt  to  the  software  side  of computer  science,  which  has  supplied  theoperating  systems,  programming  languages,  and  tools  needed  to  write  modern  programs  (andpapers about them).  But this is one area where the debt has been repaid:  work in AI has pioneeredmany  ideas  that  have  made  their  way  back  to  "mainstream"  computer  science,  including  timesharing,  interactive  interpreters,  the  linked  list  data  type,  automatic  storage  management,  andsome  of the  key  concepts  of object-oriented  programming  and  integrated  program developmentenvironments  with  graphical  user  interfaces.Linguistics (1957-present)In  1957.  B.  F.  Skinner published  Verbal  Behavior.  This  was  a  comprehensive,  detailed  accountof the  behaviorist  approach  to language  learning, written by  the  foremost  expert  in  the field.  Butcuriously,  a review  of the  book became  as well-known as  the book itself, and  served  to almost killoff interest in behaviorism.  The author of the review was  Noam Chomsky, who had just publisheda book on his own theory. Syntactic Structures.  Chomsky showed how the behaviorist theory didnot address  the  notion  of creativity  in language—it did not explain  how  a child could  understandand  make  up  sentences  that  he  or  she  had  never  heard  before.   Chomsky's  theory—based  onsyntactic  models going back  to  the Indian  linguist Panini  (c.  350 B.C.)—could explain  this,  andunlike previous theories, it  was  formal enough that  it could  in  principle be programmed.Later  developments  in linguistics  showed  the  problem  to  be  considerably  more  complexthan  it  seemed  in  1957.    Language  is  ambiguous  and  leaves  much  unsaid.    This  means  thatunderstanding  language  requires  an  understanding  of the  subject  matter  and  context,  not just  anunderstanding  of the  structure  of sentences.  This  may  seem  obvious,  but  it  was  not  appreciateduntil  the early  1960s.  Much  of the early  work in  knowledge representation  (the study  of how toput  knowledge  into  a  form  that  a  computer  can  reason with)  was  tied  to  language  and  informedby  research  in  linguistics,  which was  connected  in turn  to  decades  of work  on  the  philosophicalanalysis  of language.She  also  gave  her name  to  Ada. the  U.S.  Department of Defense's  all-purpose programming language.
     IntroductionModern linguistics and AI were "born" at about the same time, so linguistics does not playa  large  foundational  role  in  the  growth  of AI.  Instead,  the  two  grew  up  together,  intersectingin  a  hybrid  field  called  computational  linguistics  or  natural  language  processing,   whichconcentrates on the problem of language use.1.3     THE HISTORY  OF ARTIFICIAL INTELLIGENCEWith  the  background  material  behind  us,  we  are  now  ready  to  outline  the  development  of AIproper.  We could do this by identifying loosely defined and overlapping phases in its development,or by chronicling the various different and  intertwined conceptual threads that make up the field.In  this  section,  we  will  take  the former  approach,  at  the  risk  of doing  some  degree  of violenceto  the  real  relationships  among  subfields.   The  history  of  each  subfield  is  covered  in  individualchapters later in the book.The  gestation of artificial intelligence (1943-1956)The  first  work  that  is  now  generally  recognized  as  AI  was  done  by  Warren  McCulloch  andWalter  Pitts  (1943).    They  drew  on  three  sources:   knowledge  of  the  basic  physiology  andfunction  of neurons in  the  brain;  the  formal  analysis  of propositional logic  due to  Russell  andWhitehead;  and Turing's theory  of computation.  They  proposed  a model  of artificial  neurons inwhich  each  neuron  is  characterized  as  being  "on"  or  "off,"  with  a  switch  to  "on"  occurring  inresponse  to  stimulation  by  a  sufficient  number  of neighboring neurons.   The  state  of  a  neuronwas conceived of as "factually equivalent to a proposition which proposed its adequate stimulus."They  showed,  for example,  that any  computable function could  be computed by  some  networkof  connected  neurons,    and  that  all  the  logical  connectives  could  be  implemented  by  simplenet  structures.   McCulloch  and  Pitts  also  suggested  that  suitably  defined  networks  could  learn.Donald Hebb (1949) demonstrated a simple updating rule for modifying the connection strengthsbetween neurons,  such that learning could take place.The work of McCulloch and Pitts was arguably the forerunner of both the logicist tradition             iin  AI  and  the  connectionist  tradition.   In  the  early  1950s,  Claude  Shannon  (1950)  and  AlanTuring  (1953)  were  writing chess  programs  for von Neumann-style conventional  computers.12At  the  same  time,  two  graduate  students  in  the  Princeton  mathematics  department,  MarvinMinsky  and  Dean  Edmonds,  built  the  first  neural  network  computer  in  1951.   The  SNARC,  asit  was  called,  used  3000  vacuum  tubes  and  a  surplus  automatic  pilot  mechanism  from  a  B-24bomber to simulate a network of 40 neurons.  Minsky's Ph.D.  committee  was  skeptical  whetherthis kind  of work should be  considered  mathematics,  but  von Neumann  was  on the  committeeand  reportedly  said,  "If it isn't  now it will  be  someday."  Ironically,  Minsky  was  later  to provetheorems that contributed to  the demise  of much  of neural network research during the  1970s.12  Shannon  actually  had  no  real  computer to  work  with,  and  Turing  was  eventually  denied  access  to  his  own  team'scomputers by the  British government, on the grounds that research into artificial intelligence was surely frivolous.
 The  History  of Artificial  Intelligence17Princeton  was home  to another influential figure in AI,  John McCarthy.  After graduation,McCarthy  moved  to  Dartmouth  College,  which  was  to  become  the  official  birthplace  of  thefield.  McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bringtogether U.S. researchers interested in automata theory, neural nets, and the study of intelligence.They organized a two-month workshop at Dartmouth in the summer of  1956.  All together therewere  ten  attendees,  including Trenchard  More from Princeton,  Arthur  Samuel  from IBM,  andRay  Solomonoff and  Oliver  Selfridge from  MIT.Two researchers  from Carnegie Tech,13  Alien Newell  and  Herbert  Simon,  rather stole theshow.   Although  the  others  had  ideas  and  in  some  cases  programs  for  particular  applicationssuch  as checkers,  Newell  and Simon already had a reasoning program, the Logic Theorist (LT),about  which  Simon  claimed,  "We  have  invented  a  computer  program  capable  of thinking non-numerically, and thereby solved the venerable mind-body problem."14  Soon after the workshop,the  program  was  able  to  prove  most  of the  theorems  in  Chapter  2  of Russell  and Whitehead'sPrincipia Mathematica.  Russell was reportedly delighted when  Simon  showed him that the pro-gram had come up with a proof for one theorem that was  shorter than the one in Principia.  Theeditors  of the  Journal  of Symbolic Logic were  less  impressed;  they  rejected  a  paper coauthoredby Newell, Simon, and Logic Theorist.The Dartmouth workshop did not lead to any  new breakthroughs, but it did introduce allthe  major  figures  to  each  other.   For  the  next  20  years,  the  field  would  be  dominated  by  thesepeople  and  their students  and  colleagues  at  MIT,  CMU,  Stanford,  and  IBM.  Perhaps  the  mostlasting thing to come out of the workshop was an agreement to adopt McCarthy's new name forthe  field:  artificial  intelligence.Early enthusiasm, great expectations (1952-1969)The  early  years  of AI  were full  of successes—in  a  limited way.  Given the primitive computersand  programming  tools  of the time,  and  the  fact  that  only  a  few  years  earlier  computers  wereseen  as things that could do arithmetic and no more, it was astonishing whenever a computer didanything remotely clever.  The intellectual establishment, by and large, preferred to believe that "amachine can never do X" (see Chapter 26 for a long list of X's gathered by Turing).  AI researchersnaturally responded by  demonstrating  one X after another.  Some  modern AI researchers  refer tothis period as the "Look, Ma, no hands!"  era.Newell  and  Simon's  early  success  was  followed  up  with  the  General  Problem  Solver,or  GPS.   Unlike  Logic  Theorist,  this  program  was  designed  from  the  start  to  imitate  humanproblem-solving protocols.  Within the limited class  of puzzles it could handle, it turned out thatthe order in which the program considered subgoals and possible actions was similar to the wayhumans  approached  the  same  problems.  Thus,  GPS  was  probably the  first program  to  embodythe  "thinking humanly"  approach.  The  combination  of AI  and cognitive  science  has  continuedat CMU up to the present day.13   Now  Carnegie Mellon  University  (CMU).14  Newell and Simon also invented a list-processing language, IPL, to write LT.  They had no compiler, and translated itinto  machine  code by hand.  To  avoid errors,  they  worked in  parallel,  calling out binary numbers to  each  other as  theywrote each instruction to make sure they agreed.
 IntroductionAt IBM, Nathaniel Rochester  and his colleagues  produced  some  of the first AI programs.Herbert Gelernter  (1959) constructed the Geometry  Theorem  Prover.  Like the Logic Theorist,it  proved  theorems  using  explicitly  represented  axioms.   Gelernter  soon  found  that  there  weretoo many  possible reasoning paths to follow, most  of which turned out to be  dead  ends.  To helpfocus the  search,  he  added  the capability  to  create  a numerical  representation  of a  diagram—aparticular case of the general theorem to be proved.  Before the program tried to prove something,it could first check the diagram to  see if it was  true in the particular case.Starting  in  1952,  Arthur Samuel  wrote  a  series  of programs  for checkers  (draughts)  thateventually learned to play tournament-level checkers.  Along the way, he disproved the idea thatcomputers can only do what they are told to, as his program quickly learned to play a better gamethan its  creator.  The  program was demonstrated  on television in February  1956, creating  a verystrong impression.  Like Turing, Samuel had trouble finding computer time.  Working at night, heused machines that were still on the testing floor at IBM's manufacturing plant.  Chapter 5  coversgame playing, and Chapter 20 describes and expands on the learning techniques used by Samuel.John McCarthy moved from Dartmouth to MIT and there made three crucial contributionsin one historic year:  1958.  In MIT AI Lab Memo No.  1, McCarthy defined the high-level languageLisp, which was to become the dominant AI programming language.  Lisp is the second-oldestlanguage in current use.15  With Lisp, McCarthy had the tool he needed, but access to scarce andexpensive computing resources was also a serious problem.  Thus, he and others at MIT inventedtime sharing.  After getting an experimental time-sharing system up at MIT, McCarthy eventuallyattracted the interest of a group of MIT grads who formed Digital Equipment Corporation, whichwas  to  become  the world's  second  largest computer manufacturer,  thanks  to  their time-sharingminicomputers.   Also  in  1958,  McCarthy  published  a  paper  entitled  Programs  with  CommonSense,  in which he  described the Advice Taker,  a hypothetical program that can  be  seen  as thefirst complete  AI  system.  Like the Logic Theorist and  Geometry  Theorem  Prover,  McCarthy'sprogram was designed to use knowledge to search for solutions to problems. But unlike the others,it was  to  embody general knowledge  of the  world.  For example,  he  showed  how  some  simpleaxioms would enable the program to generate a plan to drive to the airport to catch a plane.  Theprogram was also designed so that it could accept new axioms in the normal course of operation,thereby allowing it to achieve competence in new areas without being reprogrammed.  The AdviceTaker thus  embodied  the  central  principles  of knowledge  representation  and  reasoning:   that  itis  useful  to  have  a  formal,  explicit representation  of the  world  and  the  way  an  agent's  actionsaffect the world,  and to be  able to manipulate these representations with deductive processes.  Itis remarkable how much  of the  1958 paper remains  relevant after more than  35  years.1958 also marked the year that Marvin Minsky moved to MIT.  For years he and McCarthywere  inseparable  as  they  defined  the  field  together.   But  they  grew  apart  as  McCarthy  stressedrepresentation  and  reasoning  in  formal  logic,  whereas  Minsky  was  more  interested  in  gettingprograms  to work,  and  eventually  developed  an  anti-logical  outlook.  In  1963,  McCarthy  tookthe  opportunity  to  go  to  Stanford  and  start  the  AI  lab  there.    His  research  agenda  of  usinglogic  to  build  the  ultimate  Advice  Taker  was  advanced  by  J.  A.  Robinson's  discovery  of theresolution method (a complete theorem-proving algorithm for first-order logic;  see Section 9.6).Work at  Stanford emphasized  general-purpose  methods for logical reasoning.  Applications  of15  FORTRAN  is one year older than Lisp.
  The  History  of Artificial  Intelligence19logic included Cordell Green's question answering and planning systems (Green,  1969b), and theShakey robotics project at the new Stanford Research Institute (SRI). The latter project, discussedfurther in  Chapter  25,  was  the  first  to  demonstrate  the  complete  integration  of logical  reasoningand physical activity.Minsky supervised a series of students who chose limited problems that appeared to requireintelligence  to solve.   These  limited domains  became  known  as  microworlds.   James  Slagle'sSAINT  program  (1963a)  was  able  to  solve  closed-form  integration problems  typical  of first-yearcollege  calculus  courses.   Tom  Evans's  ANALOGY  program  (1968)  solved  geometric  analogyproblems that appear in IQ tests,  such  as the one in Figure  1.2.  Bertram Raphael's  (1968) SIR(Semantic Information Retrieval)  was  able to  accept input statements  in a very restricted  subsetof English  and  answer  questions  thereon.   Daniel  Bobrow's  STUDENT  program  (1967)  solvedalgebra story problems  such  a